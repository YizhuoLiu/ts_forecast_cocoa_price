---
title: Cocoa Price Prediction Based on Time Series Models
author: "Yizhuo Liu, Leo Kaixuan Cheng, Haobo Ren, Betty (Ruoran) Li"
date: "April-04-2024"
output:
  pdf_document:
    toc: true
    number_sections: true
    includes:
      in_header: preamble.tex
subtitle: 
date-format: long
abstract: 
format: pdf
fig_caption: true
bibliography: references.bib
csl: apa.csl
thanks: 'Code and data are available at: https://github.com/YizhuoLiu/ts_forecast_cocoa_price'
---
```{R setup, include = FALSE}
library(dplyr)
library(ggplot2)
library(mgcv)
library(Hmisc)
library(Pmisc)
library(astsa)
library(tseries)
library(forecast)
library(GET)
library(zoo)
library(tidyr)
library(readr)
library(lubridate)
library(knitr)
knitr::opts_chunk$set(fig.pos = 'H', echo = FALSE, message = FALSE, warning = FALSE)
```

# Introduction
Cocoa is an important agricultural product, especially for some West African countries like Ghana and Côte d’Ivoire (@climate). These two nations produce more than half of the world’s cocoa supply. Therefore, changes in cocoa prices have very important effects on their economies and the income of farmers. This study aims to find the key factors that influence cocoa futures prices. The primary purpose is to develop forecasting models that help predict future price movements.

Cocoa prices often change due to external shocks. Climate change is one major factor. Studies show that abnormal weather can harm cocoa trees and reduce production. This supply shortage often causes prices to rise sharply. For example, in Ghana and Côte d’Ivoire, rising temperatures and irregular rainfall have already created serious problems for cocoa farmers (@climate). Also, inflation and changes in exchange rates can change production costs and influence market demand. When inflation increases in cocoa-importing countries or the U.S. dollar strengthens, cocoa prices become more unstable (@exchange). Another important factor is the disease of the cocoa trees, which are susceptible to pests and viruses, such as the swollen shoot virus. These diseases reduce the number of healthy trees, leading prices to increase due to lower production (@disease). All these factors are connected and must be considered together when modeling cocoa prices.

In this report, we will use time series forecasting models, including ARIMA and linear regression with exogenous variables. These models will help identify patterns of cocoa price predictions. The analysis is based on historical data and includes climate, economic, and agricultural indicators.

There are some key challenges in this study. The data may be missing or incomplete. Also, some time series are non-stationary, which means their trends change over time. Finally, combining different types of data can be very complex. To solve these problems, we will use careful preprocessing steps and test different models to choose the most accurate and reliable one.



# Literature Review
Time series forecasting has been widely used for modeling commodity prices. Classical models like ARIMA and GARCH are popular due to their simplicity. However, these models assume the linearity and stationarity of the series, which do not hold for all real-world data. An increasing number of studies have started exploring machine learning methods, which can better capture non-linear patterns of the data.

A study in 2010 used univariate ARIMA models to predict cocoa bean prices (@ARIMA). The study tested ARIMA on monthly cocoa price data. It shows that ARIMA is very effective in capturing seasonality and trends. However, it cannot directly include the influence of the explanatory variables. Its performance is not very good when data is highly influenced by other factors. Our study uses ARIMA as our preliminary model, and given the problem mentioned above, we also built an ensemble model by combining the predictions from ARIMA and linear regression to improve the forecast. ARIMA captures trends and patterns over time, while linear regression focuses on relationships of cocoa prices with external variables. By averaging the predictions of these two models, the ensemble can reduce errors and balance the strengths of both approaches.

As mentioned before, this study also uses a linear regression model on its own to forecast cocoa futures prices. Linear regression is simple and allows the participation of external variables. Although it may not capture complex time series, it intuitively shows how each factor influences the price directly. 

Another model used in this study is ARIMAX, which extends the ARIMA model by including external variables. A recent study used ARIMAX to forecast California’s energy consumption using population, production levels, energy prices, and so on (@ARIMAX). Their results show that ARIMAX gives more accurate forecasts than ARIMA, especially when external variables highly influence the response variable. This supports the idea that adding predictors can improve model performance. In our study, ARIMAX is used to include temperature, precipitation, inflation rate and disease reports in cocoa-producing countries. This is better for modelling and forecasting real-world data than the univariate ARIMA approach.

In this study, we explore a convolutional neural network (CNN) approach to forecast cocoa futures prices. CNNs are well-known in image and speech recognition, and they are increasingly used for time series forecasting. Another study proposed a WaveNet-inspired CNN model for conditional time series forecasting. Their model uses dilated convolutions to learn long-term dependencies and can make forecasts based on multiple related time series. CNN uses fewer parameters than RNNs and is faster to train (@CNN). We applied a simplified version of the temporal CNN to test whether it can capture the cocoa prices when it's affected by temperature. 

# Data


# Methodology
We split our data into a training set and a test set using a 80% - 20% ratio. Besides the daily price data, we extract a monthly and yearly time series data by averaging all price in one month or one year to see if our models are robust enough to train on those data. For NA and missing values in the dataset, we interpolate the data directly. 

## Preliminary Model - ARIMA
First of all, we have chosen ARIMA to be our Preliminary model. As mentioned in the Literature Review, ARIMA seems to be one of the best models when it comes to predicting the price of Cocoa and our initial plot of cocoa price doesn’t seem to be stationary. In order for our ARIMA model to perform better, we need to find the best degree of difference and dependence order for our data. Since our data is non-stationary we apply a first difference ($d = 1$) to our data and re-assess. If the data is stationary, we continue to find the AR and MA Orders ($p$ and $q$), and if the first difference isn’t enough to make the data stationary we will introduce a second difference into our model and so on. After determining the degree of difference, we plot the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF). A sharp cut-off in the PACF suggests the appropriate AR order ($p$) for our differenced data. And a sharp cut-off in the ACF is used to determine the MA order ($q$). Using the method above, we can find ARIMA models with appropriate degree of difference and dependence order. In general, we will write the model as
$$\Phi(B)(1 - B)^d y_t = \Theta(B) \varepsilon_t$$

where $\epsilon_t$ is the white noise sequence. In practice, we will use the function (ARIMA) in R to generate an ARIMA model for forecasting. 


## Model Ensemble - ARIMA + Linear Model
As good as our time domain approach time series model might be, it lacks consideration of influencing factors in Cocoa price such as climate change, disease and exchange rate. So to compensate for the lack of factors we decided to introduce an ensemble model which not only contains an ARIMA model, but also a linear model which includes factors influencing cocoa price as predictors. We will do the same as described in our preliminary model for the ARIMA part in the ensemble model. The linear regression model is going to be a little more complicated. Naturally we choose our response variable to be the price of the Cocoa, and our predictors to be factors that influence Cocoa price the most. This will give us a general equation for our linear model.
$$y_t=\sum_{i=1}^N\beta_i x_t^i+\epsilon$$
where $x_i^t$ are predictors $i$ at time $t$ and $\epsilon$ is the noise. The parameters of the linear model will be fitted using least square estimation (LSE). Then we will drop predictors using p-value and T-test (with alpha value being 0.05) to reduce redundancy in our model. Last but not the least, we need to combine the 2 models, because It is hard to evaluate fairly which model should have a higher weight. We decided to use a simple average approach to the ensemble model. This give us the equation for our model:
$$y_t=\frac{y^\text{ARIMA}_t+y^\text{LM}_t}{2}$$
The ensemble model is useful since our linear model and ARIMA model capture different aspects of the data and we want to combine their strengths.



## ARIMAX
The dependence of the price on other factors such as weather and diseases may be important. Therefore, we choose to use a conditional ARIMA model which is called autoregressive integrated mean average with exogenous variables (ARIMAX). It takes care of the dependence of the time series price with other time series with an expression:

$$y_t=\Phi(B)(1-B)^dy_t=\Theta(B)\epsilon_t+\mathbb{\beta}^T \boldsymbol{x}_t$$
where $\epsilon_t$ is a white noise sequence and $x_t$ is our exogenous variable. If we expand this, we can get 

$$
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} 
+ \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q} 
+ \beta_1 x_{1,t} + \beta_2 x_{2,t} + \cdots + \beta_k x_{k,t} + \epsilon_t
$$

This is a kind of conditional ARIMA, where our time series of interest is conditioned on other variables that are potentially influential. With that, we may be able to predict the price more accurately using the other variables. In practice, we use price as the $y$ variable and we treat everything else in the dataset as the $X$ variable. The ARIMA parameters are chosen based on inspection and model selection techniques such as AIC and MSE. The model is then fitted using a standard ARIMA technique. 

## Linear Regression on Lags
During our investigation on background information about Cocoa price, the most important factor which influenced the Cocoa price was the Cocoa price before. So we decided to introduce a linear regression model which contains the price of Cocoa leading to the prediction day. The method for building the linear regression on lags model is no different than our linear regression model in ensemble model. But this time we not only have final predictors in our ensemble model, but also price of Cocoa 1 day before to 10 days before. Then we will use the p-value and t-test again to determine which predictors to keep. The formula for this linear regression model is given by

$$y_t=\sum_{t=1}^{10}\alpha_ty_t+\sum_{i=1}^N\beta_ix_t^i+\epsilon$$

## Temporal CNN

Convolutional neural network (CNN) is a popular machine learning model when dealing with image and sequential data. More precisely, when it comes to univariate time series data in our case, we will perform a 1d convolution, which basically convolves some kernels with the predefined receptive field before time step $t$. After that, take the loss function as that distance between the predicted time series and our training data then backpropagate the gradient as in usual machine learning practice. CNN can easily extract temporal dependence inherently. We are using dilation on CNN to better extract the feature. 

More specifically, we are using an encoder-decoder structure. We encode the time series and the corresponding exogenous time series using temporal CNN into some hidden state. We treat different features of the time series as different channels. In this process, we also utilize recurrent neural networks (RNN) to make the network respect the long term dependency. The encoder consists of many residual blocks. Then, using multi-layer perceptrons (MLP), we construct a decoder to predict future values conditioned on the past time series as well as the exogenous variables. The model is fully differentiable so we optimize it using an AdamW optimizer with appropriate hyperparameters tuned by the validation error on a test set splitted from the dataset. 


# Results and Forecasting

```{R, include = FALSE}
data <- read.csv("Prices.csv", header = TRUE, stringsAsFactors = FALSE)
conditional_data <- read.csv("Ghana_data.csv", header = TRUE)
GH_price <- read.csv("USD_GHS Historical Data.csv", header = TRUE)

data$Date <- as.Date(data$Date, format = "%d/%m/%Y")

data$Price <- as.numeric(data$Price)

data <- data[order(data$Date), ]

n <- nrow(data)
split_index <- floor(0.8 * n)

day_full_train <- data[1:split_index, ]
day_full_test <- data[(split_index + 1):n, ]


start_date <- min(day_full_train$Date)
start_year <- as.numeric(format(start_date, "%Y"))
start_day <- as.numeric(format(start_date, "%j"))
price_ts <- ts(day_full_train$Price, start = c(start_year, start_day), frequency = 30.5)

price_test <- ts(day_full_test$Price, start = end(price_ts)[1], frequency = 30.5)

```
## Preliminary Model
From the figure we can the data after first differencing. After the first difference the data look closer to a Gaussian Noise and with a relatively constant mean, the variance also isn't changing as much with any obvious trend, therefore no heteroskedasticity issues. But, looking at the PACF of first difference data, there is no obvious cut-off suggesting a second cut-off is needed. Therefore, we decided to do a second differentiation to our data. After second differencing the data still looks stationary. Now we need to determine the order of MA(q) and AR(p).

```{R, echo = FALSE, fig.width=3, fig.height=5}
diff_sales <- diff(price_ts, lag = 1)
diff_2_sales <- diff(price_ts, lag = 2)


par(mfrow = c(3, 1))

plot(diff_sales, main = "First Difference of Sales", ylab = "Diff(Sales)")

acf(diff_sales, main = "ACF of First Differenced Series")

pacf(diff_sales, main = "PACF of First Differenced Series")

diff_2_sales <- diff(price_ts, lag = 2)

plot(diff_2_sales, main = "Second Difference of Sales", ylab = "Diff(Sales)")
acf(diff_2_sales, main = "ACF of Second Differenced Series")
pacf(diff_2_sales, main = "PACF of Second Differenced Series")

arima_fit <- arima(price_ts, order = c(0,2,1))

```

For the PACF graph, there is a cutoff right before lag = 1 and a decreasing trend; all PACF values after lag one are close to 0. The same could not be said for the ACF plot,  there is a cutoff right after lag = 0 . This suggests that, AR order should be 0 and MA order should be 1.
This leads to the following equation:

$$$$

After deciding on the dependence orders and degree of differencing, we first train our ARIMA model using the training dataset. Then use the “forecast” function in R to forecast Cocoa price based on our ARIMA(0,2,1) model. And we set our forecasting length to be the length of our testing dataset. This gives us a figure  - plot of our forecasting data and actual real world data. 

```{r, echo = FALSE, fig.width=10, fig.height=5} 
arima_forecast <- forecast(arima_fit, h = 1700)

autoplot(arima_forecast) +
  autolayer(price_test, series = "Actual") +
  labs(title = "ARIMA Forecast vs Actual", x = "Time", y = "Price")

```

The plot of our predicted value is a straight line, without any obvious seasonal change. Also, as shown in figure there is a great difference between actual data and our predicted data suggesting the MSE for our model is going to be big and our model performed poorly. Our AIC calculation backs this up, using the “summary” function in R, we got an AIC value of 61719.06. The preliminary model isn’t very complex, so it doesn’t get penalized by having many parameters. This large value of AIC might  indicate that our preliminary model has more information loss compared to other candidate models, but we will have to wait and see how the other model performs. 

## Model Ensemble - ARIMA + Linear Model

```{R, include = FALSE}
month_mapping <- data.frame(
  MonthAbbr = c("Jan", "Feb", "Mar", "Apr", "May", "Jun",
                "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"),
  diseases = c(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 9.5, 14.5, 11.0, 0.0, 0.0)
)

daily_seq <- data.frame(
  Date = seq.Date(from = as.Date("1994-01-01"),
                  to   = as.Date("2024-12-31"),
                  by   = "day")
)

daily_diseases <- daily_seq %>%
  mutate(MonthAbbr = month(Date, label = TRUE, abbr = TRUE) %>% as.character()) %>%
  left_join(month_mapping, by = "MonthAbbr") %>%
  select(Date, diseases)
price_data <- read.csv("Prices.csv", header = TRUE, stringsAsFactors = FALSE)
conditional_data <- read.csv("Ghana_data.csv", header = TRUE)
reg_temp <- read.csv("region_temp.csv", header = TRUE)
inflation <- read.csv("inflation.csv", header = TRUE)

reg_temp$Date <- as.Date(reg_temp$Date)

conditional_data$DATE <- as.Date(conditional_data$DATE)

price_data$Date <- as.Date(price_data$Date, format = "%d/%m/%Y")

inflation$Date <- as.Date(inflation$Date)

inflation <- inflation %>%
  mutate(Year = year(Date))

daily_seq <- data.frame(
  Date = seq(from = as.Date("1994-01-01"),
             to   = as.Date("2024-12-31"),
             by   = "day")
) %>%
  mutate(Year = year(Date))

inflation_daily <- daily_seq %>%
  left_join(inflation %>% select(-Date), by = "Year")

conditional_data <- conditional_data %>% select(DATE, PRCP, TAVG, TMAX, TMIN) %>% group_by(DATE) %>%
  summarise(
    avg_PRCP = mean(PRCP, na.rm = TRUE),
    avg_TAVG = mean(TAVG, na.rm = TRUE),
    avg_TMAX = mean(TMAX, na.rm = TRUE),
    avg_TMIN = mean(TMIN, na.rm = TRUE)
  )

conditional_data <- rename(conditional_data, Date = DATE)

mid_join <- inner_join(conditional_data, price_data, by = "Date")

mid_join_1 <- left_join(mid_join, inflation_daily, by = "Date")

mid_join_2 <- left_join(mid_join_1, daily_diseases, by = "Date")

GAM_data <- left_join(mid_join_2, reg_temp)

GAM_data$logMonthDays = log(Hmisc::monthDays(GAM_data$Date))
GAM_data$dateInt = as.integer(GAM_data$Date)
GAM_data$yearFac = factor(format(GAM_data$Date, "%Y"))

GAM_data <- GAM_data %>%
  arrange(avg_PRCP) %>%
  mutate(across(where(is.numeric), ~ na.approx(., na.rm = FALSE)))

GAM_data <- GAM_data %>%
  arrange(avg_TAVG) %>%
  mutate(across(where(is.numeric), ~ na.approx(., na.rm = FALSE)))

GAM_data <- GAM_data %>%
  arrange(avg_TMAX) %>%
  mutate(across(where(is.numeric), ~ na.approx(., na.rm = FALSE)))

GAM_data <- GAM_data %>%
  arrange(avg_TMIN) %>%
  mutate(across(where(is.numeric), ~ na.approx(., na.rm = FALSE)))


```

The ARIMA part of our ensemble model is going to be ARIMA(0,2,1) just like the preliminary model. As mentioned in the literature review, some of the most influential factors for Cocoa price are climate change(temperature -average, max and min ,and precipitation), inflation and diseases. Therefore, these are the factors we choose for our linear regression model. By using the “lm” function in R we are able to produce the linear regression model. Then we calculate p value for all of the predictors. (figure) , based on the figure and our alpha value being 0.05, we decided to drop the predictor maximum temperature.

```{R, lag, include = FALSE}
create_lags <- function(data, lags = 1:7) {
  for (lag in lags) {
    data[[paste0("lag_", lag)]] <- dplyr::lag(data$Price, lag)
  }
  return(data)
}
GAM_data <- GAM_data[order(GAM_data$Date), ]

GAM_data_lag <- create_lags(GAM_data)
train_size <- floor(0.8 * nrow(GAM_data_lag))
train_lm <- GAM_data_lag[1:train_size, ]
test_lm <- GAM_data_lag[(train_size + 1):nrow(GAM_data_lag), ]

train_ts <- train_lm %>% select(Date, Price)
test_ts <- test_lm %>% select(Date, Price)

train_ts$Date <- as.Date(train_lm$Date)
test_ts$Date <- as.Date(test_ts$Date)

train_ts <- train_ts[order(train_ts$Date), ]
test_ts <- test_ts[order(test_ts$Date), ]


start_year <- as.numeric(format(train_ts$Date[1], "%Y"))
start_day <- as.numeric(format(train_ts$Date[1], "%j"))

train_ts <- ts(train_ts$Price, frequency = 91.5)

start_year <- as.numeric(format(test_ts$Date[1], "%Y"))
start_day <- as.numeric(format(test_ts$Date[1], "%j"))
test_ts <- ts(test_ts$Price, frequency = 91.5)

```

```{R, include = FALSE}

lm_model <- lm(Price ~ avg_PRCP + avg_TAVG + avg_TMAX + avg_TMIN + Inflation + diseases +Reg_temp, data = train_lm)

lm_model <- lm(Price ~ avg_PRCP + avg_TAVG  + avg_TMIN + Inflation + diseases +Reg_temp, data = train_lm)

lm_model_pre <- predict(lm_model, newdata = test_lm, type = "response")

```

After getting the ARIMA and linear regression model we desired, we can now produce the ensemble model using the formula in the method section. Then we can forecast Cocoa price using our ensemble model on the testing dataset, this gives the figure . Looking at the graph below we see that there is a seasonal trend in our forecasting data but the overall trend of Cocoa price is not observed. This is an improvement on the preliminary model, but not by much, we still need to explore more options.

```{r, echo = FALSE, fig.width=10, fig.height=5}

arima_model <- arima(train_ts, order = c(0,2,1))

arima_forecast <- forecast(arima_model, h = 1334)

arima_pred <- arima_forecast$mean

combined_pred <- (lm_model_pre + arima_pred) / 2

two_results <- tibble(
  Date = test_lm$Date,
  Actual = test_lm$Price,
  Predicted = combined_pred
)

ggplot(two_results, aes(x = Date)) +
  geom_line(aes(y = Actual), color = "red") +
  geom_line(aes(y = Predicted), color = "blue") +
  labs(title = "Ensemble model Forecast vs Actual Prices", y = "Price", x = "Date") +
  theme_minimal()

```

## ARIMAX

The ARIMAX model is fitted using daily data as well as the monthly data with parameter chosen automatically. The inference results on the monthly and daily data can be seen from the diagram below.  We can see that the result is relatively bad although we take into consideration other factors. It is similar to the results with a vanilla ARIMA model: the predicted time series becomes straight as a line, the error bar is enormous compared to the scale of the time series. The overall upward trend is correct except for the drastically increasing. This shows that the model is very uncertain on its prediction, and simply cannot fit the data because of its complexity. This suggests that ARIMAX is not a good choice for this data. 

The final ARIMAX model chosen for daily data based on AIC is ARIMA(0,1,0), showing that it fits very bad on the data since the data is highly non-stationary and the bset ARIMA model even does not depend on the past. For monthly data, the model is ARIMA(0,1,1). 


```{R}
day_full <- read_csv("final_day_date.csv", na = "0", show_col_types = FALSE)
day_full <- day_full %>% select(-logMonthDays, -dateInt, -yearFac, -Annual...Change)
day_full <- day_full[order(day_full$Date), ]

day_full <- day_full %>%
  mutate(across(where(is.character), ~ suppressWarnings(as.numeric(.)))) 

day_full <- day_full %>%
  arrange(Date) %>%
  mutate(across(where(is.numeric), ~ na.approx(., na.rm = FALSE)))

n <- nrow(day_full)
split_index <- floor(0.8 * n)

day_full_train <- day_full[1:split_index, ]
day_full_test <- day_full[(split_index + 1):n, ]
```

```{R, echo=FALSE}
price_train = ts(day_full_train %>% select("Price"), frequency = 365)
reg_clean_train = day_full_train %>%
  select(-Date, -Price) %>%
  as.matrix()

reg_clean_test = day_full_test %>%
  select(-Date, -Price) %>%
  as.matrix()

# model = Arima(price_train, order=c(1,2,1), xreg = reg_clean_train)
model = auto.arima(price_train, xreg = reg_clean_train)

price_test_day <- ts(day_full_test$Price, start = end(price_train)[1] + 1, frequency = 365)

forecast_result_day = forecast(model, h = 10, xreg = reg_clean_test)

```


```{R}
month_full <- read_csv("month_data_full.csv", show_col_types = FALSE)

month_full <- month_full %>%
  mutate(across(where(is.character), ~ as.numeric(.)))

month_full <- month_full %>%
  arrange(Date) %>%
  mutate(across(where(is.numeric), ~ na.approx(., na.rm = FALSE)))
```
```{R}
n <- nrow(month_full)
split_index <- floor(0.8 * n)

month_full_train <- month_full[1:split_index, ]
month_full_test <- month_full[(split_index + 1):n, ]
```

```{R, echo=FALSE}
price_train = ts(month_full_train %>% select("Price"), frequency = 12)
reg_clean_train = month_full_train %>%
  select(-Date, -Price) %>%
  as.matrix()

reg_clean_test = month_full_test %>%
  select(-Date, -Price) %>%
  as.matrix()

# model = Arima(price_train, order=c(6,2,1), xreg = reg_clean_train)
# summary(model)
model = auto.arima(price_train, xreg = reg_clean_train)

price_test <- ts(month_full_test$Price, start = end(price_train)[1] + 1, frequency = 12)

forecast_result = forecast(model, h = 1, xreg = reg_clean_test)
```
```{R}
#| fig-cap: ARIMAX model fitted on daily data.
library(gridExtra)


par(mfrow=c(1,2)) 
# Plot forecast vs actual
p1 = autoplot(forecast_result_day) +
  autolayer(price_test_day, series = "Actual") +
  labs(title = "ARIMAX Forecast on Daily Data", x = "Time", y = "Price")

# Plot forecast vs actual
p2 = autoplot(forecast_result) +
  autolayer(price_test, series = "Actual") +
  labs(title = "ARIMA Forecast on Monthly Data", x = "Time", y = "Price")

grid.arrange(p1, p2, ncol = 2)
```


## Linear Regression on Lags
```{R, include =  FALSE}
GAM_data_lag_1 <- GAM_data_lag %>% 
  select(Date, Price, starts_with("lag_"), avg_PRCP , avg_TAVG , avg_TMAX , avg_TMIN , Inflation , diseases , Reg_temp)

train_size <- floor(0.8 * nrow(GAM_data_lag_1))
train_lm_lag <- GAM_data_lag_1[1:train_size, ]
test_lm_lag <- GAM_data_lag_1[(train_size + 1):nrow(GAM_data_lag_1), ]

lm_model_lag <- lm(Price ~ ., 
                   data = train_lm_lag%>%select(-Date))

lm_model_lag <- lm(Price ~ lag_1 + lag_4 + lag_5, 
                   data = train_lm_lag%>%select(-Date))

lm_pre_lag <- predict(lm_model_lag, newdata = test_lm_lag, type = "response")

```
After exploring the previous 3 models, we are able to find that one of the most influential factors for Cocoa price is the Cocoa price before. With this finding in mind, we decided to add lag in price, and use it as predictors. Using the “lm” function in R and our training dataset, we are able to produce our new linear regression model. Then we use t-test and p-value to find which of the lag prices is actually significant. Turns out only lag 1(price of cocoa 1 day before our prediction day) , lag 4 and lag 5 is significant for our model.  Using lag1,2,5 we are able to produce a linear regression model on the training data set, and produce a forecast for price on the testing dataset. Below is the actual value of the Cocoa price with our predicted Cocoa price in the model. The predicted and actual price seems to be equivalent in most cases, resulting in the blue line and the red line overlapping each other. This is an indication for a very good prediction, but there is still improvement for the model since linear regression often isn’t the top choice for time sensitive data.


```{R, echo = FALSE, fig.width=10, fig.height=5}

### Plot Regression Predictions
lm_result_lag <- tibble(
  Date = test_lm_lag$Date,
  Actual = test_lm_lag$Price,
  Predicted = lm_pre_lag
)

lm_rmse <- sqrt(mean((lm_result_lag$Actual - lm_result_lag$Predicted)^2))

lm_rmse

ggplot(lm_result_lag, aes(x = Date)) +
  geom_line(aes(y = Actual), color = "red") +
  geom_line(aes(y = Predicted), color = "blue") +
  labs(title = "Linear Regression Forecast vs Actual Prices", y = "Price", x = "Date") +
  theme_minimal()

```

## Temporal CNN
It takes the CNN ~1000 epochs to converge to a low loss. The fitted time series on the training and test set are shown. We can see that the model performs extremely well on the training set, showing that the expressiveness of the model is sufficient to capture the training data. We can see that the trend of the time series is correctly predicted by the CNN, but there is still some deviations from the true test set. The validation MSE loss on the test set is also large. A potential reason is that the model is too complex and it overfits the given training set even though we apply a standard $L^2$ regularization  on its weights. Overly complex model may not be suitable for a time series prediction without good regularization. The loss curve is also shown below. 

![](CNN training set.png)
```{r fig.twoimages, echo=FALSE, fig.cap="Side-by-side comparison of image A and image B.", out.width="49%"} 

par(mfrow=c(1,2)) 
knitr::include_graphics(c("CNN training loss.png", "CNN test loss.png")) 
```


# Discussion
Overall the traditional time series prediction methods all perform relatively bad on the cocoa global price dataset. This shows that this data has an inherently complex nature, and many models are bad at capturing the pattern of the data. Another factor that may be problematic is the stationarity of the time series, which is hard to achieve. Also, traditional models such as ARIMA may fail because they don’t take into consideration the exogenous variables such as weather and inflation. To resolve this, we found that the linear model trained on lags achieved a satisfying performance on both the training set and the generalization to the test set. Theoretically, temporal CNN will also perform well on this the performance of the linear model. However, due to issues like overfitting, the CNN model is not good as the linear model when predicting the future. 

Although the vanilla ensemble model does not perform well, we may use the idea to combine two equally good models and tune the weight between the outputs of the two models to gain a better result. Or we can use another function to dynamically combine the results given by two or more models. 

Data quality may also be important. We may incorporate more relevant data to train our models. 

Recently there are works on utilizing diffusion models for time series prediction [@diffusion], which achieve state-of-the-art performance. Using that, we may be able to fit a robust and well-generalized model on this complex dataset conditioned on other factors. The flexibility of the proposed model enables us to do that.

# Conclusion

# Reference
